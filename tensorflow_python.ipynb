{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0518 12:15:11.904128 15944 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Karan.Arya\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0518 12:15:28.006524 15944 deprecation.py:323] From C:\\Users\\Karan.Arya\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0518 12:15:31.145538 15944 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: These Regulations may cited Personal Data ProtectionRegulations 2014 shall come operation 2nd July 2014. ########################## Similar sentences: This Act may cited Personal Data Protection Act 2012and shall come operation date Minister may, bynotification Gazette, appoint. ########## Notwithstanding provisions Part, anorganisation may use personal data individual collectedbefore appointed day purposes personal datawas collected unless —(a) consent use withdrawn accordance withsection 16; or(b) individual, whether before, appointedday, otherwise indicated organisation doesnot consent use personal data.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tika import parser\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# Request PDF URLs\n",
    "\n",
    "url_2012 = 'https://sso.agc.gov.sg/Act/PDPA2012?ViewType=Pdf'\n",
    "# url_2012 = input('Enter the URL of the first PDF: ')\n",
    "response_2012 = requests.get(url_2012)\n",
    "\n",
    "url_2014 = 'https://sso.agc.gov.sg//SL/PDPA2012-S362-2014?DocDate=20140519&ViewType=Pdf&_=20180520142357'\n",
    "# url_2014 = input('Enter the URL of the second PDF: ')\n",
    "response_2014 = requests.get(url_2014)\n",
    "\n",
    "\n",
    "# Save URL content to PDF locally\n",
    "\n",
    "with open(\"sample_2012.pdf\",'wb') as f: \n",
    "    f.write(response_2012.content)\n",
    "\n",
    "with open(\"sample_2014.pdf\",'wb') as f: \n",
    "    f.write(response_2014.content) \n",
    "\n",
    "\n",
    "# Parsing text from PDF using tika\n",
    "\n",
    "text_2012 = parser.from_file('sample_2012.pdf')\n",
    "text_2014 = parser.from_file('sample_2014.pdf')\n",
    "\n",
    "# Stopwords removal using NLTK\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Cleaning text of PDF from the year 2014\n",
    "# Meanwhile, specific to the document\n",
    "# Must be generalized for broad usage\n",
    "# Removal of new lines\n",
    "# Removal of footers\n",
    "# Removal of headers\n",
    "# Removal of headings - text containing 'definitions' text\n",
    "# Removal of unnecessary text during initial and end pages of the PDF\n",
    "# Removal of excessively small sentences\n",
    "# Stopwords removal\n",
    "\n",
    "sentences_2014 = sent_tokenize(text_2014['content'])\n",
    "sentences_2014 = [sent.replace('\\n', '') for sent in sentences_2014]\n",
    "sentences_2014 = sentences_2014[15:]\n",
    "sentences_2014 = [sent.replace('Informal Consolidation – version in force from 2/7/2014', '') for sent in sentences_2014]\n",
    "sentences_2014 = [sent.replace('S 362/2014', '') for sent in sentences_2014]\n",
    "sentences_2014 = [sent for sent in sentences_2014 if 'Definitions of this Part' not in sent]\n",
    "sentences_2014 = sentences_2014[:42]\n",
    "sentences_2014 = [sent for sent in sentences_2014 if len(sent) > 20]\n",
    "sentences_2014 = [' '.join(w for w in sent.split() if w not in stop_words) for sent in sentences_2014]\n",
    "\n",
    "# Cleaning text of PDF from the year 2012\n",
    "# Meanwhile, specific to the document\n",
    "# Must be generalized for broad usage\n",
    "# Removal of new lines\n",
    "# Removal of footers\n",
    "# Removal of headers\n",
    "# Removal of unnecessary text during initial and end pages of the PDF\n",
    "# Removal of excessively small sentences\n",
    "# Stopwords removal\n",
    "\n",
    "sentences_2012 = sent_tokenize(text_2012['content'])\n",
    "sentences_2012 = [sent.replace('\\n', '') for sent in sentences_2012]\n",
    "sentences_2012 = sentences_2012[74:]\n",
    "sentences_2012 = [sent.replace('Informal Consolidation – version in force from 2/10/2016', '') for sent in sentences_2012]\n",
    "sentences_2012 = [sent.replace('26 OF 2012', '') for sent in sentences_2012]\n",
    "sentences_2012 = sentences_2012[:361]\n",
    "sentences_2012 = [sent for sent in sentences_2012 if len(sent) > 20]\n",
    "sentences_2012 = [' '.join(w for w in sent.split() if w not in stop_words) for sent in sentences_2012]\n",
    "\n",
    "# Using Universal Sentence Encoder from TensorFlowHub\n",
    "# Computing the similarity of a sample sentence from 2014 PDF to all sentences from 2012 PDF\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_sentences_encodings = embed(similarity_input_placeholder)\n",
    "result_list = []\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    sentences_embeddings_2012 = session.run(similarity_sentences_encodings, feed_dict={similarity_input_placeholder: sentences_2012})\n",
    "    sentences_embeddings_2014 = session.run(similarity_sentences_encodings, feed_dict={similarity_input_placeholder: sentences_2014})\n",
    "    for i in range(len(sentences_2012)):\n",
    "        similarity = np.inner(sentences_embeddings_2014[0], sentences_embeddings_2012[i])\n",
    "        if similarity > 0.8:\n",
    "            result_list.append(sentences_2012[i])\n",
    "\n",
    "\n",
    "# Returning a sample sentence from 2014 PDF and its similar sentences from 2012 PDF\n",
    "\n",
    "print('Sample sentence: {} ########################## Similar sentences: {} ########## {}'.format(sentences_2014[0], result_list[0], result_list[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
